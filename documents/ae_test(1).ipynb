{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTrjs1NihWZB",
        "outputId": "026bde1b-8fdd-4980-9092-123f5f9da7ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "cHheNvdYsz_O"
      },
      "outputs": [],
      "source": [
        "#!pip install tf-nightly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "vdVflhXzfIfR",
        "outputId": "c06c9501-e1b0-4a1e-f42e-73e9a8331c5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.8.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "tf.test.gpu_device_name()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "D-4e9Pn8cP6m"
      },
      "outputs": [],
      "source": [
        "#o = []\n",
        "#while True:\n",
        "#  o.append(o)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROjJFOuHk_eg",
        "outputId": "45ad4939-c98a-4726-b749-f70e1b84c312"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "167430609062_elasticloadbalancing_eu-west-1_app.modanisa-external.c514a951a6e0bfb3_20220101T0000Z_52.211.21.47_4ue5n8nd.log\n",
            "167430609062_elasticloadbalancing_eu-west-1_app.modanisa-external.c514a951a6e0bfb3_20220101T0240Z_54.154.31.214_38es3f9f.log\n",
            "167430609062_elasticloadbalancing_eu-west-1_app.modanisa-external.c514a951a6e0bfb3_20220101T0250Z_52.211.21.47_4wgzifnz.log\n",
            "167430609062_elasticloadbalancing_eu-west-1_app.modanisa-external.c514a951a6e0bfb3_20220101T0255Z_52.211.21.47_zm3l802v.log\n",
            "167430609062_elasticloadbalancing_eu-west-1_app.modanisa-external.c514a951a6e0bfb3_20220101T0305Z_52.211.21.47_5g2c0cn7.log\n",
            "167430609062_elasticloadbalancing_eu-west-1_app.modanisa-external.c514a951a6e0bfb3_20220101T0310Z_52.211.21.47_59fkgo77.log\n",
            "167430609062_elasticloadbalancing_eu-west-1_app.modanisa-external.c514a951a6e0bfb3_20220101T0310Z_54.154.31.214_7agoebkc.log\n",
            "167430609062_elasticloadbalancing_eu-west-1_app.modanisa-external.c514a951a6e0bfb3_20220101T0320Z_52.211.21.47_3gbz4mur.log\n",
            "167430609062_elasticloadbalancing_eu-west-1_app.modanisa-external.c514a951a6e0bfb3_20220101T0320Z_54.154.31.214_158338lm.log\n",
            "167430609062_elasticloadbalancing_eu-west-1_app.modanisa-external.c514a951a6e0bfb3_20220101T0325Z_52.211.21.47_1v2ewn00.log\n",
            "167430609062_elasticloadbalancing_eu-west-1_app.modanisa-external.c514a951a6e0bfb3_20220101T0325Z_54.154.31.214_3acyax32.log\n",
            "167430609062_elasticloadbalancing_eu-west-1_app.modanisa-external.c514a951a6e0bfb3_20220101T0330Z_52.211.21.47_17k7uuc9.log\n",
            "167430609062_elasticloadbalancing_eu-west-1_app.modanisa-external.c514a951a6e0bfb3_20220101T0340Z_52.211.21.47_2hmb3n3k.log\n",
            "167430609062_elasticloadbalancing_eu-west-1_app.modanisa-external.c514a951a6e0bfb3_20220101T0340Z_54.154.31.214_3cmmuu8x.log\n",
            "167430609062_elasticloadbalancing_eu-west-1_app.modanisa-external.c514a951a6e0bfb3_20220101T0355Z_54.154.31.214_3ujhhli1.log\n",
            "167430609062_elasticloadbalancing_eu-west-1_app.modanisa-external.c514a951a6e0bfb3_20220101T0400Z_52.211.21.47_4qfgr38n.log\n",
            "167430609062_elasticloadbalancing_eu-west-1_app.modanisa-external.c514a951a6e0bfb3_20220101T0400Z_54.154.31.214_5xj7adz5.log\n",
            "167430609062_elasticloadbalancing_eu-west-1_app.modanisa-external.c514a951a6e0bfb3_20220101T0405Z_54.154.31.214_17q6qaom.log\n",
            "167430609062_elasticloadbalancing_eu-west-1_app.modanisa-external.c514a951a6e0bfb3_20220101T0410Z_52.211.21.47_2a1xe0v1.log\n",
            "167430609062_elasticloadbalancing_eu-west-1_app.modanisa-external.c514a951a6e0bfb3_20220101T0445Z_52.211.21.47_2vjucf2j.log\n",
            "167430609062_elasticloadbalancing_eu-west-1_app.modanisa-external.c514a951a6e0bfb3_20220101T0445Z_54.154.31.214_3epdm2d6.log\n",
            "167430609062_elasticloadbalancing_eu-west-1_app.modanisa-external.c514a951a6e0bfb3_20220101T0450Z_52.211.21.47_3i19zywt.log\n",
            "167430609062_elasticloadbalancing_eu-west-1_app.modanisa-external.c514a951a6e0bfb3_20220101T0450Z_54.154.31.214_3mz74m47.log\n",
            "167430609062_elasticloadbalancing_eu-west-1_app.modanisa-external.c514a951a6e0bfb3_20220101T0500Z_52.211.21.47_202xk2es.log\n",
            "167430609062_elasticloadbalancing_eu-west-1_app.modanisa-external.c514a951a6e0bfb3_20220101T0500Z_54.154.31.214_5mz3n4mi.log\n",
            "167430609062_elasticloadbalancing_eu-west-1_app.modanisa-external.c514a951a6e0bfb3_20220101T0505Z_54.154.31.214_1lb7nk92.log\n",
            "167430609062_elasticloadbalancing_eu-west-1_app.modanisa-external.c514a951a6e0bfb3_20220101T0520Z_54.154.31.214_48ks5wqy.log\n",
            "167430609062_elasticloadbalancing_eu-west-1_app.modanisa-external.c514a951a6e0bfb3_20220101T0525Z_54.154.31.214_1o6suc65.log\n",
            "167430609062_elasticloadbalancing_eu-west-1_app.modanisa-external.c514a951a6e0bfb3_20220101T0530Z_52.211.21.47_1aiwi3xk.log\n",
            "167430609062_elasticloadbalancing_eu-west-1_app.modanisa-external.c514a951a6e0bfb3_20220101T0540Z_52.211.21.47_1m5nu9zs.log\n",
            "167430609062_elasticloadbalancing_eu-west-1_app.modanisa-external.c514a951a6e0bfb3_20220101T0550Z_52.211.21.47_4vajbj3v.log\n",
            "167430609062_elasticloadbalancing_eu-west-1_app.modanisa-external.c514a951a6e0bfb3_20220101T0600Z_54.154.31.214_2ayq7kcc.log\n",
            "167430609062_elasticloadbalancing_eu-west-1_app.modanisa-external.c514a951a6e0bfb3_20220101T0605Z_52.211.21.47_5besj3fw.log\n",
            "167430609062_elasticloadbalancing_eu-west-1_app.modanisa-external.c514a951a6e0bfb3_20220101T0605Z_54.154.31.214_1sw9rn12.log\n",
            "167430609062_elasticloadbalancing_eu-west-1_app.modanisa-external.c514a951a6e0bfb3_20220101T0620Z_54.154.31.214_1awwtf55.log\n",
            "167430609062_elasticloadbalancing_eu-west-1_app.modanisa-external.c514a951a6e0bfb3_20220101T0625Z_52.211.21.47_4brbn23l.log\n",
            "167430609062_elasticloadbalancing_eu-west-1_app.modanisa-external.c514a951a6e0bfb3_20220101T0635Z_52.211.21.47_2exsqu6a.log\n",
            "167430609062_elasticloadbalancing_eu-west-1_app.modanisa-external.c514a951a6e0bfb3_20220101T0755Z_52.211.21.47_1ff95gnt.log\n"
          ]
        }
      ],
      "source": [
        "!ls \"/content/drive/MyDrive/ae-test-datasets\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "avfnIC2ZYvaz"
      },
      "outputs": [],
      "source": [
        "#!pip install --upgrade tensorflow \n",
        "#from datasketch import MinHash, MinHashLSH\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_1BLvlyd6y_Z"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "#import gensim\n",
        "#from gensim.models import Word2Vec\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "from urllib.parse import urlparse,unquote\n",
        "import time\n",
        "#import nltk\n",
        "#nltk.download('punkt')\n",
        "\n",
        "\n",
        "global vocabulary\n",
        "vocabulary = {}\n",
        "vocabulary[\"_pad_\"]=str(0)\n",
        "vocabulary[\"_one_char_\"]=str(4)\n",
        "vocabulary[\"_others_\"]=str(2)\n",
        "vocabulary[\"_one_dig_\"]=str(5)\n",
        "vocabulary[\"_eos_\"]=str(3)\n",
        "vocabulary[\"_sos_\"]=str(1)\n",
        "vocabulary[\"_num_\"]=str(6)\n",
        "vocabulary[\"_char_num_\"]=str(7)\n",
        "global vocab_i\n",
        "global vocfq\n",
        "vocfq = {}\n",
        "vocfq[\"_pad_\"]=0\n",
        "vocfq[\"_one_char_\"]=0\n",
        "vocfq[\"_others_\"]=0\n",
        "vocfq[\"_one_dig_\"]=0\n",
        "vocfq[\"_eos_\"]=0\n",
        "vocfq[\"_sos_\"]=0\n",
        "vocfq[\"_num_\"]=0\n",
        "vocfq[\"_char_num_\"]=0\n",
        "vocab_i = 8\n",
        "stringhashes = {}\n",
        "traininginputs = {}\n",
        "ddirectories = {}\n",
        "dqueries = {}\n",
        "trainees = []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "LLRp1XW2koIf"
      },
      "outputs": [],
      "source": [
        "#http logs for offline training\n",
        "def getinputfile(file):\n",
        "    rev = pd.read_csv(file, delim_whitespace=True,header=None)\n",
        "    return rev\n",
        "\n",
        "#get the related part from logs DF\n",
        "def processfile(data):\n",
        "    processed = []\n",
        "    for i,_ in enumerate(data):\n",
        "        #replaced = re.sub(\",|\\||%|/|\\.|\\?|&|-|=|\\+|:|_|}|{|!|\\$|'|\\(|\\)|\\*\",\" \",_)\n",
        "        #replaced = ' '.join(e for e in _ if e.isalnum())\n",
        "        #print(_)\n",
        "        replaced = str(_)\n",
        "        replaced = unquote(replaced)\n",
        "        replaced = re.sub('[^A-Za-z0-9]+', ' ', replaced)\n",
        "        replaced = re.sub(\"(?<!\\S)\\d(?!\\S)\", \" _one_dig_ \",replaced)\n",
        "        replaced = re.sub(\"(^|\\s+)\\w(?=\\s+|$)\", \" _one_char_ \", replaced)  \n",
        "        replaced = re.sub(\"\\\\b(\\\\d+)\\\\b\", \" _num_ \", replaced)\n",
        "        #replaced = re.sub(\"\\\\b([a-zA-Z]\\\\d+)\\\\b\", \" _char_num_ \",replaced)\n",
        "        #print(replaced)\n",
        "        processed.append(\"_sos_ \"+replaced + \" _eos_\"+\" _pad_\")\n",
        "    return processed\n",
        "\n",
        "#parse payload\n",
        "def parsepayload(payload):\n",
        "    \n",
        "    return\n",
        "\n",
        "trainingdata = {}\n",
        "#tokenize training corpus\n",
        "def tokenizetraining(data):\n",
        "    data = pd.Series(data)\n",
        "    tok = []\n",
        "    for i,_ in enumerate(data):\n",
        "        #print(_)\n",
        "        while unquote(_) != _:\n",
        "            _ = unquote(_) \n",
        "        x = tokenizepayload(_)\n",
        "        rev = mapt2int(x,vocabulary)\n",
        "              \n",
        "        if rev not in tok:\n",
        "            tok.append(rev)\n",
        "        \n",
        "    return tok\n",
        "\n",
        "#def tokenizetraining(data):\n",
        "#    tok = []\n",
        "#    for i, _ in enumerate(data):\n",
        "#        res=[]\n",
        "#        print(_)\n",
        "#        rev = _.split()\n",
        "#        for x in rev:\n",
        "#            res.append(c.wv.word_vec(x.lower()))\n",
        "#            \n",
        "#        tok.append(res)\n",
        "#    return tok\n",
        "#prepare corpus\n",
        "def preparecorpus(rev):\n",
        "    processed = processfile(rev[:])\n",
        "    #proc = rev[:100000][5]\n",
        "    corpus_text = ' '.join(processed)\n",
        "    data = []\n",
        "    # iterate through each sentence in the file\n",
        "    for i in sent_tokenize(corpus_text):\n",
        "        temp = []\n",
        "        # tokenize the sentence into words\n",
        "        for j in word_tokenize(i):\n",
        "            temp.append(j.lower())\n",
        "        data.append(temp)\n",
        "        #data=processfile(data)\n",
        "    return data, processed\n",
        "\n",
        "def preparevocabulary(corpus):\n",
        "    global vocabulary, vocab_i\n",
        "    for _ in corpus:\n",
        "        #for _x in _:\n",
        "        if _ in vocabulary:\n",
        "            vocfq[_] += 1\n",
        "            continue\n",
        "        else:\n",
        "            vocabulary[_] = str(vocab_i)\n",
        "            vocfq[_]=1\n",
        "            vocab_i += 1\n",
        "    return\n",
        "\n",
        "\n",
        "#embedding to create vocabulary\n",
        "#def embedwords(parsedpayload):\n",
        "#    model = Word2Vec(parsedpayload, size=5,min_count = 1, window = 5, sg=0,workers=8) \n",
        "#    return model\n",
        "\n",
        "#tokenize the newly arrived http request\n",
        "def tokenizepayload(payload):\n",
        "    #print(payload)\n",
        "    payload=str(payload)\n",
        "    payload = payload.lower()\n",
        "    replaced = unquote(payload)\n",
        "    replaced = re.sub('[^A-Za-z0-9]+', ' ', replaced)\n",
        "    replaced = re.sub(\"(?<!\\S)\\d(?!\\S)\", \" _one_dig_ \",replaced)\n",
        "    replaced = re.sub(\"(^|\\s+)\\w(?=\\s+|$)\", \" _one_char_ \", replaced) \n",
        "    replaced = re.sub(\"\\\\b(\\\\d+)\\\\b\", \" _num_ \", replaced)\n",
        "    #replaced = re.sub(\"\\\\b([a-zA-Z]\\\\d+)\\\\b\", \" _char_num_ \",replaced)\n",
        "    splitted = replaced.split()\n",
        "    if len(splitted)>490:\n",
        "        replaced = \" \".join(splitted[:250])\n",
        "    replaced = \"_sos_ \"+ replaced + \" _eos_\"\n",
        "    #print(replaced)\n",
        "    #replaced = \"\".join(replaced)\n",
        "    \n",
        "    #print(replaced)\n",
        "    return replaced\n",
        "\n",
        "#def processpayload(data,w2vmodel):\n",
        "#    processed = []\n",
        "#    for i,_ in enumerate(data):\n",
        "        #replaced = re.sub(\",|\\||%|/|\\.|\\?|&|-|=|\\+|:|_|}|{|!|\\$|'|\\(|\\)|\\*\",\" \",_)\n",
        "        #replaced = ' '.join(e for e in _ if e.isalnum())\n",
        "        #print(_)\n",
        "#        replaced = unquote(_)\n",
        "#        replaced = re.sub('[^A-Za-z0-9]+', ' ', replaced)\n",
        "#        replaced = re.sub(\"(?<!\\S)\\d(?!\\S)\", \" _one_dig_ \",replaced)\n",
        "#        replaced = re.sub(\"(^|\\s+)\\w(?=\\s+|$)\", \" _one_char_ \", replaced)\n",
        "#        replaced = re.sub(\"\\\\b(\\\\d+)\\\\b\", \" _num_ \", replaced)\n",
        "        #replaced = re.sub(\"\\\\b([a-zA-Z]\\\\d+)\\\\b\", \" _char_num_ \",replaced)\n",
        "        #print(replaced)\n",
        "#        p = \"_sos_ \"+replaced + \" _eos_\"\n",
        "#        p=p.lower().split()\n",
        "#        p_count = 80-len(p)\n",
        "#        p=p+[\"_pad_\"]*p_count\n",
        "#        pp = [(w2vmodel.wv.get_vector(x)).tolist() for x in p]\n",
        "#        processed.append(np.asarray(pp).reshape(80,5))\n",
        "        \n",
        "#    return processed  \n",
        "\n",
        "#embed online inputs\n",
        "def mapt2int(data,vocab):\n",
        "    #print(data)\n",
        "    #data = tokenizepayload(data)\n",
        "    #print(data)\n",
        "    #for key, value in vocab.items():\n",
        "        #print(key)\n",
        "    #print(\"Data : \"+str(data))\n",
        "    #dd=[]\n",
        "    for word in data.split():\n",
        "        \n",
        "        if word in vocab:\n",
        "            data = data.replace(word, str(\" \"+vocab[word])+\" \",1)\n",
        "        else: \n",
        "            data = data.replace(word, \" 2 \",1)\n",
        "    return data\n",
        "\n",
        "def prepareforml(data):\n",
        "    mldata = []\n",
        "\n",
        "    if len(data)==1:\n",
        "        mldata.append(data[0].split())\n",
        "        return mldata\n",
        "    else:\n",
        "        for _ in data:\n",
        "            mldata.append(_.split())\n",
        "        return mldata\n",
        "\n",
        "def preparetraininoutput(data):\n",
        "    preprocessed =[]\n",
        "    aa = [0]*len(vocabulary)\n",
        "    for _ in data:\n",
        "        processed =[]\n",
        "        #print(_)\n",
        "        for x in _.split():\n",
        "            dd = aa\n",
        "            dd[int(x)]=1\n",
        "            processed.append(dd)\n",
        "        preprocessed.append(processed)\n",
        "    return preprocessed\n",
        "\n",
        "def parseurls(a):\n",
        "    if not isinstance(a, pd.Series):\n",
        "        a=pd.Series(a)    \n",
        "    headers= []\n",
        "    for _ in a:\n",
        "        url = _.split()[1]\n",
        "        #while \"%\" in url or not stopiter:\n",
        "        while unquote(url) != url:\n",
        "            url = unquote(url)            \n",
        "        headers.append(url_parser(url))\n",
        "    return headers\n",
        "\n",
        "\n",
        "def url_parser(url):\n",
        "    \n",
        "    parts = urlparse(url)\n",
        "    directories = parts.path.strip('/').split('/')\n",
        "    unamp = re.sub(\"&amp;\", \"&\" ,parts.query)\n",
        "    queries = unamp.strip('&').split('&')\n",
        "    queriesdict = dict((s.split('=')+[1])[:2] for s in queries)\n",
        "    \n",
        "    elements = {\n",
        "        'scheme': parts.scheme,\n",
        "        'netloc': parts.netloc,\n",
        "        'path': parts.path,\n",
        "        'params': parts.params,\n",
        "        'query': parts.query,\n",
        "        'fragment': parts.fragment,\n",
        "        'directories': directories,\n",
        "        'queries': queries,\n",
        "        'queriesdict': queriesdict,\n",
        "    }\n",
        "    \n",
        "    return elements\n",
        "\n",
        "traininginputs = {}\n",
        "#def preparefortraining(data):\n",
        "#    print(len(data))\n",
        "#    ax = []\n",
        "#    i=0\n",
        "#    for _ in data:\n",
        "#        trainingarr =[]\n",
        "#        parsed = tokenizepayload(_)\n",
        "#        splitted = parsed.split()\n",
        "        #splitted_len = len(splitted)\n",
        "        #print(splitted_len)\n",
        "        # if splitted_len not in traininginputs:\n",
        "        #     trainingarr.append(_)\n",
        "        #     traininginputs[splitted_len]=trainingarr\n",
        "        #     ax.append(_)\n",
        "        # elif splitted_len in traininginputs:\n",
        "        #     if _ in traininginputs[splitted_len]:\n",
        "        #         continue\n",
        "        #     elif _ not in traininginputs[splitted_len]:\n",
        "        #         trainingarr.append(_)\n",
        "        #         traininginputs[splitted_len].append(trainingarr)\n",
        "        #         ax.append(_)\n",
        "        #for c, i in enumerate(data):\n",
        "        #    t = time.time()\n",
        "#        minhash = MinHash(num_perm=64)\n",
        "#        for d in tokenizepayload(_).split():\n",
        "            #print(d)\n",
        "#            minhash.update(d.encode('utf-8'))\n",
        "            #print(c)\n",
        "#        result = lsh.query(minhash)\n",
        "        #print(_)\n",
        "        #print(result)\n",
        "#        if len(result)==0:\n",
        "            #print(\"no entry\")\n",
        "#            lsh.insert(i, minhash)\n",
        "#            traininginputs[i]=minhash\n",
        "#            i +=1\n",
        "#            ax.append(_)\n",
        "#        if len(result)>0:\n",
        "            #print(\"entry\")\n",
        "#            ax.append(_)\n",
        "#    print(i)\n",
        "#    return pd.Series(ax)\n",
        "def preparecorpusandvocabulary(headers):\n",
        "    corpus=set()\n",
        "    for _ in headers:\n",
        "        #print(_)\n",
        "        # for x in _[\"scheme\"]:\n",
        "        #     y = parsewords(x)\n",
        "        #     for x in y.split():\n",
        "        #print(_[\"scheme\"])\n",
        "        corpus.add(_[\"scheme\"])\n",
        "        #for x in _[\"netloc\"]:\n",
        "        y = parsewords(_[\"netloc\"])\n",
        "        for x in y.split():\n",
        "            corpus.add(x)\n",
        "        for x in _[\"directories\"]:\n",
        "            y = parsewords(x)\n",
        "            for x in y.split():\n",
        "                corpus.add(x)\n",
        "        for x in _[\"queries\"]:\n",
        "            #print(x)\n",
        "            #print(x.partition(\"=\")[0])\n",
        "            y = parsewords(x.partition(\"=\")[0])\n",
        "            for z in y.split():\n",
        "                corpus.add(y)\n",
        "    preparevocabulary(corpus)\n",
        "    return corpus\n",
        "\n",
        "#tokenize the newly arrived http request\n",
        "def parsewords(payload):\n",
        "    #print(payload)\n",
        "    payload=str(payload)\n",
        "    payload = payload.lower()\n",
        "    #replaced = unquote(payload)\n",
        "    replaced = re.sub('[^A-Za-z0-9]+', ' ', payload)\n",
        "    replaced = re.sub(\"(?<!\\S)\\d(?!\\S)\", \" _one_dig_ \",replaced)\n",
        "    replaced = re.sub(\"(^|\\s+)\\w(?=\\s+|$)\", \" _one_char_ \", replaced) \n",
        "    replaced = re.sub(\"\\\\b(\\\\d+)\\\\b\", \" _num_ \", replaced)\n",
        "    #replaced = re.sub(\"\\\\b([a-zA-Z]\\\\d+)\\\\b\", \" _char_num_ \",replaced)\n",
        "    #print(replaced)\n",
        "    #replaced = \"\".join(replaced)\n",
        "    \n",
        "    #print(replaced)\n",
        "    return replaced\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ryjTG3rh2lY1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "\n",
        "for root, dirs, files in os.walk(\"/content/drive/MyDrive/ae-test-datasets\"):\n",
        "   path = root.split(os.sep)\n",
        "   for index, file in enumerate(files):\n",
        "      im2 = [ f for f in listdir(root) if isfile(join(root,f)) ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "HjzUJ8dfZE7L"
      },
      "outputs": [],
      "source": [
        "#a1 = getinputfile(\"/content/drive/MyDrive/167430609062_elasticloadbalancing_eu-west-1_app.modanisa-external.c514a951a6e0bfb3_20220101T0000Z_52.211.21.47_4ue5n8nd.log\")\n",
        "#a1 = a1[:][12]\n",
        "#a2 = getinputfile(\"/content/drive/MyDrive/167430609062_elasticloadbalancing_eu-west-1_app.modanisa-external.c514a951a6e0bfb3_20220101T0400Z_52.211.21.47_4qfgr38n.log\")\n",
        "#a2 = a2[:][12]\n",
        "#a3 = getinputfile(\"/content/drive/MyDrive/167430609062_elasticloadbalancing_eu-west-1_app.modanisa-external.c514a951a6e0bfb3_20220101T0530Z_52.211.21.47_1aiwi3xk.log\")\n",
        "#a3 = a3[:][12]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1tANhQ43CZ0",
        "outputId": "58edab36-f7a3-46bf-a674-4be0344137cb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        }
      ],
      "source": [
        "a = pd.Series()\n",
        "for _ in im2[:]:\n",
        "  x = getinputfile(\"/content/drive/MyDrive/ae-test-datasets/\"+_)\n",
        "  a = pd.concat([a,x[:][12]],ignore_index=True, sort=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "f5dAvs5L4Um7"
      },
      "outputs": [],
      "source": [
        "trainees=[]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZQWvdX8oQvJ",
        "outputId": "460c9cd6-6738-4586-d093-f214010170cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "343.60554790496826\n",
            "0.0003825817675352894\n"
          ]
        }
      ],
      "source": [
        "def create_dict(l, d):\n",
        "    if len(l) == 1:\n",
        "        #print(\"L :\"+ str(l)+ \"...\"+str(type(l)))\n",
        "        #d=dict(d)\n",
        "        d[l[0]] = {}\n",
        "    else:\n",
        "        #print(\"D : \"+str(d) +\"...\")\n",
        "        #d = dict(d)\n",
        "        if l[0] == \"customers\":\n",
        "            l[1] = \"c_id\"\n",
        "        if l[0] == \"products\":\n",
        "            l[1] = \"p_id\"\n",
        "        if l[0] == \"product-alternative-colors\":\n",
        "            l[1] = \"p_color_id\"\n",
        "        #d = d.setdefault(l[0], {})\n",
        "        d[l[0]]={}\n",
        "        create_dict(l[1:], d[l[0]])\n",
        "\n",
        "def check_existence(k, d):\n",
        "    # we will use a way to get a dictionary about url parts like /customers or /products\n",
        "    if len(k)==1:\n",
        "        if k[0] in d:\n",
        "            if d[k[0]] == {}:\n",
        "                return True\n",
        "            else: \n",
        "                return False\n",
        "        else: \n",
        "            return False\n",
        "    else:\n",
        "        if k[0] not in d:\n",
        "            return False\n",
        "        else:\n",
        "            if k[0] == \"customers\":\n",
        "                k[1] = \"c_id\"\n",
        "            if k[0] == \"products\":\n",
        "                k[1] = \"p_id\"\n",
        "            if k[0] == \"product-alternative-colors\":\n",
        "                k[1] = \"p_color_id\"\n",
        "            return check_existence(k[1:], d[k[0]])\n",
        "\n",
        "def createqdict(k):\n",
        "    exist = False\n",
        "    for _qkey in k:\n",
        "        if _qkey != \"\":\n",
        "            if \"|\" in str(k[_qkey]):\n",
        "                k[_qkey] = k[_qkey].split(\"|\")\n",
        "            if not isinstance(k[_qkey],list):\n",
        "                k[_qkey] = [k[_qkey]]\n",
        "            #if  isinstance(ss[\"queriesdict\"][_qkey], int):\n",
        "            #ss[\"queriesdict\"][str(_qkey)]=set([ss[\"queriesdict\"][_qkey]])\n",
        "            if not str(_qkey) in dqueries:\n",
        "                exist = False\n",
        "                dqueries[str(_qkey)]=set()\n",
        "                dqueries[str(_qkey)].update(set(k[_qkey]))\n",
        "            else:\n",
        "                if set(k[_qkey]).difference(dqueries[str(_qkey)]) != set():\n",
        "                    exist = False\n",
        "                else:\n",
        "                    exist =True\n",
        "                dqueries[str(_qkey)].update(set(k[_qkey]))\n",
        "    return exist\n",
        "    \n",
        "t = time.time()\n",
        "for _ in a:\n",
        "    ss =unquote(_.split()[1])\n",
        "    while unquote(ss) != ss:\n",
        "        ss = unquote(ss)\n",
        "    ss = url_parser(ss)\n",
        "    #_dict = list_to_dict(ss[\"directories\"])\n",
        "    #print(ss[\"directories\"])\n",
        "    ex = check_existence(ss[\"directories\"], ddirectories)\n",
        "    if not ex:\n",
        "        #trainees.append(_)\n",
        "        create_dict(ss[\"directories\"],ddirectories)  \n",
        "    qex = createqdict(ss[\"queriesdict\"])\n",
        "    if not qex and not ex:\n",
        "        trainees.append(_)\n",
        "    # for _qkey in ss[\"queriesdict\"]:\n",
        "    #     if _qkey != \"\":\n",
        "    #         if \"|\" in str(ss[\"queriesdict\"][_qkey]):\n",
        "    #             ss[\"queriesdict\"][_qkey] = ss[\"queriesdict\"][_qkey].split(\"|\")\n",
        "    #         if not isinstance(ss[\"queriesdict\"][_qkey],list):\n",
        "    #             ss[\"queriesdict\"][_qkey] = [ss[\"queriesdict\"][_qkey]]\n",
        "    #         #if  isinstance(ss[\"queriesdict\"][_qkey], int):\n",
        "    #         #ss[\"queriesdict\"][str(_qkey)]=set([ss[\"queriesdict\"][_qkey]])\n",
        "    #         if not str(_qkey) in dqueries:\n",
        "    #             dqueries[str(_qkey)]=set()\n",
        "    #             dqueries[str(_qkey)].update(set(ss[\"queriesdict\"][_qkey]))\n",
        "    #         else:\n",
        "    #             dqueries[str(_qkey)].update(set(ss[\"queriesdict\"][_qkey]))\n",
        "           \n",
        "print(time.time()-t)\n",
        "print((time.time()-t)/len(a))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7tIc88TQise",
        "outputId": "01d917b4-4c8b-4ffd-ee7c-8fa90b8436eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "898124\n",
            "194111\n"
          ]
        }
      ],
      "source": [
        "print(len(a))\n",
        "print(len(trainees))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "-4TouPRiZDP9"
      },
      "outputs": [],
      "source": [
        "#a = preparefortraining(a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "gy7v3747bJNm"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Dense, Input,LSTM, TimeDistributed,RepeatVector\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "DvvawTGqbPYL"
      },
      "outputs": [],
      "source": [
        "MAX_LEN = 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "oqdqbi6kbaLK"
      },
      "outputs": [],
      "source": [
        "def decoder_output_creater(decoder_input_data, num_samples, MAX_LEN, VOCAB_SIZE):\n",
        "  \n",
        "  decoder_output_data = np.zeros((num_samples, MAX_LEN, VOCAB_SIZE), dtype=\"float32\")\n",
        "\n",
        "  for i, seqs in enumerate(decoder_input_data):\n",
        "      for j, seq in enumerate(seqs):\n",
        "          if j > 0:\n",
        "              decoder_output_data[i][j][seq] = 1.\n",
        "  #print(decoder_output_data.shape)\n",
        "  \n",
        "  return decoder_output_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "1oU5jHdFGFsE"
      },
      "outputs": [],
      "source": [
        "#a = pd.concat([a1,a2,a3],ignore_index=True, sort=False)\n",
        "b = parseurls(a)\n",
        "\n",
        "#MAX_LEN = max([len(_.split()) for _ in f])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "kfa8tyJx1jdK"
      },
      "outputs": [],
      "source": [
        "c = preparecorpusandvocabulary(b)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "rls7xY7f0MXx"
      },
      "outputs": [],
      "source": [
        "reverse_vocab = dict((i, word) for word, i in vocabulary.items())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "w403nS608xrc"
      },
      "outputs": [],
      "source": [
        " #zzz = tokenizetraining(trainees)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "J1nAOcJJiVrX"
      },
      "outputs": [],
      "source": [
        "#from tensorflow.keras.optimizers import Adam\n",
        "#ADAM = Adam(lr=0.05, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.5, amsgrad=False, clipnorm=1.)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "XRudJiFLcVKl"
      },
      "outputs": [],
      "source": [
        "def input_generator(batchsize,adata):\n",
        "   i=0\n",
        "   inputs = []\n",
        "   targets = []\n",
        "   while True:\n",
        "       f = tokenizetraining(adata[i*batchsize:(i+1)*batchsize]) \n",
        "       g = prepareforml(f)\n",
        "       padded_docs = pad_sequences(g, maxlen=MAX_LEN, padding='post')\n",
        "       dec_out = decoder_output_creater(padded_docs, len(g), MAX_LEN, len(vocabulary))\n",
        "       i +=1\n",
        "       yield padded_docs, dec_out\n",
        "       \n",
        "       i = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "xVze0H99k-Be"
      },
      "outputs": [],
      "source": [
        "#b,d = preparecorpus(a)\n",
        "#e = preparevocabulary(b)\n",
        "\n",
        "\n",
        "#c = embedwords(b)\n",
        "#c.train(processfile(a[:100000][5]), total_examples=c.corpus_count, epochs=5)\n",
        "#e = preparevocabulary(b)\n",
        "#f = pd.Series(f)\n",
        "#MAX_LEN = 935\n",
        "# import time\n",
        "# t = time.time()\n",
        "#f = tokenizetraining(a[:1500000])\n",
        "# print(time.time()-t)\n",
        "\n",
        "# t = time.time()\n",
        "# h = preparetraininoutput(f)\n",
        "# print(time.time()-t)\n",
        "# #y= prepareforml(h)\n",
        "# #f = [[c.wv.getc_] for _ in processfile(a[:100000][5])]\n",
        "# #from gensim.models import KeyedVectors\n",
        "\n",
        "\n",
        "def encoder_decoder_model_with_attension(\n",
        "        encoder_vocab_size,\n",
        "        decoder_vocab_size,\n",
        "        max_encoder_seq_length,\n",
        "        max_decoder_seq_length):\n",
        "    #  define length of encoder/decoder input\n",
        "    encoder_input = Input(shape=(max_encoder_seq_length, ))\n",
        "    #decoder_input = Input(shape=(max_decoder_seq_length, ))\n",
        "    # Encoder\n",
        "    # we can add BatchNormalization, Masking\n",
        "    encoder_input2embedding = Embedding(input_dim=encoder_vocab_size, output_dim=64, input_length=max_encoder_seq_length, mask_zero=True)(encoder_input)\n",
        "    embedding2encoder = LSTM(units=256, return_sequences=True, dropout=0.3)(encoder_input2embedding)\n",
        "    embedding2encoder2 = LSTM(units=128, return_sequences=False, dropout=0.3)(embedding2encoder)\n",
        "    repeat2encoder = RepeatVector(MAX_LEN)(embedding2encoder2)\n",
        "    # Decoder\n",
        "    # we can add BatchNormalization, Masking\n",
        "    decoderlstm2 = LSTM(units=128, return_sequences=True, dropout=0.3)(repeat2encoder)\n",
        "    decoderlstm = LSTM(units=256, return_sequences=True, dropout=0.3)(decoderlstm2)\n",
        "    context2softmax = Dense(units=decoder_vocab_size, activation=\"softmax\")\n",
        "    softmax2output = TimeDistributed(context2softmax)(decoderlstm)\n",
        "    # Attention Encoder-Decoder Model\n",
        "    model = Model(inputs=[encoder_input], outputs=[softmax2output])\n",
        "    return model\n",
        "\n",
        "model = encoder_decoder_model_with_attension(len(vocabulary), len(vocabulary), MAX_LEN, MAX_LEN)\n",
        "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\",metrics=[\"acc\"])\n",
        "#model.compile(optimizer=ADAM, loss=\"categorical_crossentropy\",metrics=[\"acc\"])\n",
        "\n",
        "#from keras.utils.vis_utils import plot_model\n",
        "#plot_model(model)\n",
        "\n",
        "\n",
        "\n",
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def data_spliter(encoder_input_data, decoder_input_data, test_size1=0.2, test_size2=0.3):\n",
        "  \n",
        "  en_train, en_test, de_train, de_test = train_test_split(encoder_input_data, decoder_input_data, test_size=test_size1)\n",
        "  en_train, en_val, de_train, de_val = train_test_split(en_train, de_train, test_size=test_size2)\n",
        "  \n",
        "  yield en_train, en_val, en_test, de_train, de_val, de_test\n",
        "\n",
        "#en_train, en_val, en_test, de_train, de_val, de_test = data_spliter(padded_docs, dec_out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6KrZ4w0RcdWY",
        "outputId": "608789ab-98ef-4e30-91a6-0022ebc04db0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 256)]             0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, 256, 64)           820288    \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 256, 256)          328704    \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 128)               197120    \n",
            "                                                                 \n",
            " repeat_vector (RepeatVector  (None, 256, 128)         0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               (None, 256, 128)          131584    \n",
            "                                                                 \n",
            " lstm_3 (LSTM)               (None, 256, 256)          394240    \n",
            "                                                                 \n",
            " time_distributed (TimeDistr  (None, 256, 12817)       3293969   \n",
            " ibuted)                                                         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,165,905\n",
            "Trainable params: 5,165,905\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gl-E3V6lpLb",
        "outputId": "0a560e2a-96d9-432a-ba30-4f5e4f752004"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "1010/1010 [==============================] - 1301s 1s/step - loss: 0.2744 - acc: 0.9379\n",
            "Epoch 2/4\n",
            "1010/1010 [==============================] - 1282s 1s/step - loss: 0.0632 - acc: 0.9737\n",
            "Epoch 3/4\n",
            "1010/1010 [==============================] - 1279s 1s/step - loss: 0.0315 - acc: 0.9859\n",
            "Epoch 4/4\n",
            "1010/1010 [==============================] - 1284s 1s/step - loss: 0.0151 - acc: 0.9918\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f6c3eeb6b50>"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(input_generator(192,trainees), steps_per_epoch=(len(trainees) / 192),epochs=4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "HbaA7fyGr0XP"
      },
      "outputs": [],
      "source": [
        "tf.keras.models.save_model(model,'/content/drive/MyDrive/ae220302.h5',overwrite=True, include_optimizer=True, save_format=None,\n",
        "    signatures=None, options=None, save_traces=True\n",
        ") "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "6lToe6tkSYtT"
      },
      "outputs": [],
      "source": [
        "model.save('/content/drive/MyDrive/ae220302-x.h5')\n",
        "model.save_weights('/content/drive/MyDrive/ae220302-x-weights.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CRD_ABmi9hVo",
        "outputId": "924d769b-1f4a-49c0-c943-e459eec8f9b0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "44"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gc\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbMhtRZxje2v",
        "outputId": "a4e2fad6-e7e8-4fab-a55d-d15a135d2e92"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/backend.py:450: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n",
            "  warnings.warn('`tf.keras.backend.set_learning_phase` is deprecated and '\n"
          ]
        }
      ],
      "source": [
        "from tensorflow import keras\n",
        "model = keras.models.load_model('/content/drive/MyDrive/ae220302.h5')\n",
        "keras.backend.set_learning_phase(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "kAG9mErYYt7N"
      },
      "outputs": [],
      "source": [
        "datasettotest = getinputfile(\"/content/drive/MyDrive/ae-test-datasets/167430609062_elasticloadbalancing_eu-west-1_app.modanisa-external.c514a951a6e0bfb3_20220101T0755Z_52.211.21.47_1ff95gnt.log\")\n",
        "datasettotest = datasettotest[12]\n",
        "#parsetest = parseurls(datasettotest)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AH2MTbaSabID"
      },
      "outputs": [],
      "source": [
        "parsetest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "Z70izwOIoMx2",
        "outputId": "9de03299-bb63-4357-83ff-82c6c42954b1"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'GET https://fcdn.modanisa.com:443/r/pro2/2020/11/06/u-desenli-kemer-detayli-kaban--siyah-mor--tesse-8073104-2.jpg HTTP/2.0'"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "datasettotest[20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HqC0EhFmZ6UG"
      },
      "outputs": [],
      "source": [
        "f = tokenizetraining(datasettotest[0]) \n",
        "g = prepareforml(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-h5Blcxa6-4"
      },
      "outputs": [],
      "source": [
        "padded_docs = pad_sequences(g, maxlen=MAX_LEN, padding='post')\n",
        "dec_out = decoder_output_creater(padded_docs, len(g), MAX_LEN, len(vocabulary))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zNcluQyLbe9f",
        "outputId": "fa1a7c40-5ad7-4be5-a602-d06737b83c59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "75.18403482437134\n"
          ]
        }
      ],
      "source": [
        "t= time.time()\n",
        "sent = []\n",
        "inp_sent = []\n",
        "toks= []\n",
        "rectoks=[]\n",
        "for _ in attacklist:\n",
        "  tokened = tokenizetraining(_)\n",
        "  toks.append(tokened)\n",
        "  toml = prepareforml(tokened)\n",
        "  padded = pad_sequences(toml, maxlen=MAX_LEN, padding='post')\n",
        "  pred = model.predict(padded)\n",
        "  sentence = \"\"\n",
        "  recs = []\n",
        "  for i in range(len(pred[0])):\n",
        "    y = np.argmax(pred[0][i])\n",
        "    recs.append(y)\n",
        "    try:\n",
        "      sentence+=(\" \"+reverse_vocab[str(y)])\n",
        "    except:\n",
        "      sentence += (\" \"+\"_others_\")   \n",
        "  rectoks.append(recs) \n",
        "  sent.append(sentence)\n",
        "  inp_sentence = \"\"\n",
        "  for _ in tokened[0].split():\n",
        "    try:\n",
        "      inp_sentence +=(\" \"+reverse_vocab[str(_)])\n",
        "    except:\n",
        "      inp_sentence += (\" \"+\"_others_\")\n",
        "  inp_sent.append(inp_sentence)\n",
        "print(time.time()-t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "jJvmRGkbmLWM"
      },
      "outputs": [],
      "source": [
        "xxx= pd.DataFrame([toks,rectoks]).T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "3jqNF_M0mVLk",
        "outputId": "ddb94860-b7af-4aa7-de98-ccfe7a3fd998"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-34abab9f-16b1-463f-ace9-16fa76296aa2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[ 1   5712   10352   2   10559   12729   8342 ...</td>\n",
              "      <td>[5712, 5712, 10352, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[ 1   5712   10352   2   10559   12729   4352 ...</td>\n",
              "      <td>[12364, 12364, 10352, 5659, 10559, 12729, 6, 1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[ 1   5712   10352   2   10559   12729   6623 ...</td>\n",
              "      <td>[5712, 5712, 10352, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[ 1   5712   10352   2   10559   12729   4352 ...</td>\n",
              "      <td>[12364, 12364, 10352, 5659, 10559, 12729, 6, 1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[ 1   5712   10352   2   10559   12729   4352 ...</td>\n",
              "      <td>[12364, 12364, 10352, 5659, 10559, 12729, 6, 1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>716</th>\n",
              "      <td>[ 1   5712   10352   2   10559   12729   4352 ...</td>\n",
              "      <td>[12364, 5712, 10352, 5659, 10559, 12729, 6, 10...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>717</th>\n",
              "      <td>[ 1   5712   10352   2   10559   12729   4352 ...</td>\n",
              "      <td>[12364, 5712, 10352, 5659, 10559, 12729, 6, 10...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>718</th>\n",
              "      <td>[ 1   5712   10352   2   10559   12729   4352 ...</td>\n",
              "      <td>[12364, 5712, 10352, 5659, 10559, 12729, 6, 10...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>719</th>\n",
              "      <td>[ 1   5712   10352   2   10559   12729   4352 ...</td>\n",
              "      <td>[12364, 5712, 10352, 5659, 10559, 12729, 6, 10...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>720</th>\n",
              "      <td>[ 1   5712   10352   2   10559   12729   4352 ...</td>\n",
              "      <td>[12364, 5712, 10352, 5659, 10559, 12729, 6, 10...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>721 rows  2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-34abab9f-16b1-463f-ace9-16fa76296aa2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-34abab9f-16b1-463f-ace9-16fa76296aa2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-34abab9f-16b1-463f-ace9-16fa76296aa2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                     0                                                  1\n",
              "0    [ 1   5712   10352   2   10559   12729   8342 ...  [5712, 5712, 10352, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
              "1    [ 1   5712   10352   2   10559   12729   4352 ...  [12364, 12364, 10352, 5659, 10559, 12729, 6, 1...\n",
              "2    [ 1   5712   10352   2   10559   12729   6623 ...  [5712, 5712, 10352, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
              "3    [ 1   5712   10352   2   10559   12729   4352 ...  [12364, 12364, 10352, 5659, 10559, 12729, 6, 1...\n",
              "4    [ 1   5712   10352   2   10559   12729   4352 ...  [12364, 12364, 10352, 5659, 10559, 12729, 6, 1...\n",
              "..                                                 ...                                                ...\n",
              "716  [ 1   5712   10352   2   10559   12729   4352 ...  [12364, 5712, 10352, 5659, 10559, 12729, 6, 10...\n",
              "717  [ 1   5712   10352   2   10559   12729   4352 ...  [12364, 5712, 10352, 5659, 10559, 12729, 6, 10...\n",
              "718  [ 1   5712   10352   2   10559   12729   4352 ...  [12364, 5712, 10352, 5659, 10559, 12729, 6, 10...\n",
              "719  [ 1   5712   10352   2   10559   12729   4352 ...  [12364, 5712, 10352, 5659, 10559, 12729, 6, 10...\n",
              "720  [ 1   5712   10352   2   10559   12729   4352 ...  [12364, 5712, 10352, 5659, 10559, 12729, 6, 10...\n",
              "\n",
              "[721 rows x 2 columns]"
            ]
          },
          "execution_count": 99,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "xxx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "59Lkpa78hUVK"
      },
      "outputs": [],
      "source": [
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "S_bfyUC5hXEp"
      },
      "outputs": [],
      "source": [
        "s1 = [re.sub(\"_pad_\",\"\",_).split() for _ in sent]\n",
        "s2 = [re.sub(\"_pad_\",\"\",_).split() for _ in inp_sent]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0XgSdZmIhnqS",
        "outputId": "543ccacc-7d34-4e89-935b-ecbf5b421a8c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ]
        }
      ],
      "source": [
        "scores=[]\n",
        "inp = []\n",
        "rec=[]\n",
        "score = []\n",
        "for i in range(len(s1)):\n",
        "  s=nltk.translate.bleu_score.sentence_bleu([s1[i]], s2[i])\n",
        "  scores.append(s)\n",
        "  if s < 0.5:\n",
        "    inp.append(s1[i])\n",
        "    rec.append(s2[i])\n",
        "    score.append(s)\n",
        "import pandas as pd\n",
        "xx = pd.DataFrame([inp,rec,score]).T\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "n6nl3_vbZXQC",
        "outputId": "59c8ade0-1662-453f-b325-a7ba1f116ed3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-0ecee720-c358-458a-b1b4-52551d807bc6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[get, get, https]</td>\n",
              "      <td>[_sos_, get, https, _others_, modanisa, com, e...</td>\n",
              "      <td>0.408248</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[delete, delete, https, api1, modanisa, com, _...</td>\n",
              "      <td>[_sos_, get, https, _others_, modanisa, com, c...</td>\n",
              "      <td>0.385386</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[get, get, https]</td>\n",
              "      <td>[_sos_, get, https, _others_, modanisa, com, a...</td>\n",
              "      <td>0.408248</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[delete, delete, https, api1, modanisa, com, _...</td>\n",
              "      <td>[_sos_, get, https, _others_, modanisa, com, c...</td>\n",
              "      <td>0.385386</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[delete, delete, https, api1, modanisa, com, _...</td>\n",
              "      <td>[_sos_, get, https, _others_, modanisa, com, c...</td>\n",
              "      <td>0.385386</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>595</th>\n",
              "      <td>[delete, get, https, api1, modanisa, com, _num...</td>\n",
              "      <td>[_sos_, get, https, _others_, modanisa, com, c...</td>\n",
              "      <td>0.438113</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>596</th>\n",
              "      <td>[delete, get, https, api1, modanisa, com, _num...</td>\n",
              "      <td>[_sos_, get, https, _others_, modanisa, com, c...</td>\n",
              "      <td>0.438113</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>597</th>\n",
              "      <td>[delete, get, https, api1, modanisa, com, _num...</td>\n",
              "      <td>[_sos_, get, https, _others_, modanisa, com, c...</td>\n",
              "      <td>0.438113</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>598</th>\n",
              "      <td>[delete, get, https, api1, modanisa, com, _num...</td>\n",
              "      <td>[_sos_, get, https, _others_, modanisa, com, c...</td>\n",
              "      <td>0.452985</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>599</th>\n",
              "      <td>[delete, get, https, api1, modanisa, com, _num...</td>\n",
              "      <td>[_sos_, get, https, _others_, modanisa, com, c...</td>\n",
              "      <td>0.452985</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>600 rows  3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0ecee720-c358-458a-b1b4-52551d807bc6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0ecee720-c358-458a-b1b4-52551d807bc6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0ecee720-c358-458a-b1b4-52551d807bc6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                     0  ...         2\n",
              "0                                    [get, get, https]  ...  0.408248\n",
              "1    [delete, delete, https, api1, modanisa, com, _...  ...  0.385386\n",
              "2                                    [get, get, https]  ...  0.408248\n",
              "3    [delete, delete, https, api1, modanisa, com, _...  ...  0.385386\n",
              "4    [delete, delete, https, api1, modanisa, com, _...  ...  0.385386\n",
              "..                                                 ...  ...       ...\n",
              "595  [delete, get, https, api1, modanisa, com, _num...  ...  0.438113\n",
              "596  [delete, get, https, api1, modanisa, com, _num...  ...  0.438113\n",
              "597  [delete, get, https, api1, modanisa, com, _num...  ...  0.438113\n",
              "598  [delete, get, https, api1, modanisa, com, _num...  ...  0.452985\n",
              "599  [delete, get, https, api1, modanisa, com, _num...  ...  0.452985\n",
              "\n",
              "[600 rows x 3 columns]"
            ]
          },
          "execution_count": 96,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "xx\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TLVlmH2XVH9F"
      },
      "outputs": [],
      "source": [
        "global vocabulary\n",
        "vocabulary = {}\n",
        "vocabulary[\"_pad_\"]=str(0)\n",
        "vocabulary[\"_one_char_\"]=str(4)\n",
        "vocabulary[\"_others_\"]=str(2)\n",
        "vocabulary[\"_one_dig_\"]=str(5)\n",
        "vocabulary[\"_eos_\"]=str(3)\n",
        "vocabulary[\"_sos_\"]=str(1)\n",
        "vocabulary[\"_num_\"]=str(6)\n",
        "vocabulary[\"_char_num_\"]=str(7)\n",
        "global vocab_i\n",
        "global vocfq\n",
        "vocfq = {}\n",
        "vocfq[\"_pad_\"]=0\n",
        "vocfq[\"_one_char_\"]=0\n",
        "vocfq[\"_others_\"]=0\n",
        "vocfq[\"_one_dig_\"]=0\n",
        "vocfq[\"_eos_\"]=0\n",
        "vocfq[\"_sos_\"]=0\n",
        "vocfq[\"_num_\"]=0\n",
        "vocfq[\"_char_num_\"]=0\n",
        "vocab_i = 8\n",
        "stringhashes = {}\n",
        "traininginputs = {}\n",
        "ddirectories = {}\n",
        "dqueries = {}\n",
        "trainees = []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "1qCUmtCyzE-S"
      },
      "outputs": [],
      "source": [
        "import pickle\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "B2S88QQVWkqh"
      },
      "outputs": [],
      "source": [
        "varsss = [vocabulary,vocfq,vocab_i,vocfq,dqueries,ddirectories]\n",
        "varsstr = [\"vocabulary\",\"vocfq\",\"vocab_i\",\"dqueries\",\"ddirectories\"]\n",
        "\n",
        "for i in range(len(varsss)):\n",
        "  with open('/content/drive/MyDrive/variables'+str(varsstr[i])+'.pickle', 'wb') as f:\n",
        "    pickle.dump(varsss[i],f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "_Kum5FTjXxTz"
      },
      "outputs": [],
      "source": [
        "with open('/content/drive/MyDrive/variablesvocabulary.pickle', 'rb') as f:\n",
        "   vocabulary =pickle.load(f)\n",
        "with open('/content/drive/MyDrive/variablesvocfq.pickle', 'rb') as f:\n",
        "   vocfq =pickle.load(f)\n",
        "with open('/content/drive/MyDrive/variablesvocab_i.pickle', 'rb') as f:\n",
        "   vocab_i =pickle.load(f)\n",
        "\n",
        "with open('/content/drive/MyDrive/variablesdqueries.pickle', 'rb') as f:\n",
        "   dqueries =pickle.load(f)\n",
        "with open('/content/drive/MyDrive/variablesddirectories.pickle', 'rb') as f:\n",
        "   ddirectories =pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "e4lQDQoVX5Ei",
        "outputId": "d95461a0-cb92-4e2d-8ded-459957db7d19"
      },
      "outputs": [
        {
          "ename": "ParserError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-76-b2105f9268de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0matacktest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/ae-test-datasets/RAW2.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1045\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m         \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 1 fields in line 3, saw 67\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "atacktest = pd.read_csv(\"/content/drive/MyDrive/ae-test-datasets/RAW2.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "rrTiL0_J7Q1f"
      },
      "outputs": [],
      "source": [
        "### Loop the data lines\n",
        "with open(\"/content/drive/MyDrive/ae-test-datasets/RAW2.csv\", 'r') as temp_f:\n",
        "    # get No of columns in each line\n",
        "    col_count = [ len(l.split(\",\")) for l in temp_f.readlines() ]\n",
        "\n",
        "### Generate column names  (names will be 0, 1, 2, ..., maximum columns - 1)\n",
        "column_names = [i for i in range(0, max(col_count))]\n",
        "\n",
        "### Read csv\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/ae-test-datasets/RAW2.csv\", header=None, delimiter=\",\", names=column_names)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "vqAIP7vp2sUS"
      },
      "outputs": [],
      "source": [
        "dfcsv= df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "id": "uuaeSIOYNZ9W",
        "outputId": "43e45f86-4b40-4022-fe4d-e2c29d00e31f"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-c314140a4a02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m';'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5485\u001b[0m         ):\n\u001b[1;32m   5486\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5487\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5489\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Series' object has no attribute 'splitlines'"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "result = [row for row in csv.reader(dfcsv.splitlines(), delimiter=';')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "NN2Msq2zN75k"
      },
      "outputs": [],
      "source": [
        "dflist=dfcsv.to_list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "nkStzOYOQ8CQ"
      },
      "outputs": [],
      "source": [
        "attacklist = []\n",
        "for _ in dflist:\n",
        "  x = _[0][1:-1] +\" \"+_[1][1:-1]+\"://\"+_[2][1:-1]+_[3][1:-1]+\"?\"+_[4][1:-1]\n",
        "  attacklist.append(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sw8tA0IAObBk",
        "outputId": "243466a3-8fc0-42b9-c786-a47fa9ad4904"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "721"
            ]
          },
          "execution_count": 97,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(attacklist)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "ae_test.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
